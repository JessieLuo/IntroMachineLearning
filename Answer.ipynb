{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 75 Âµs (started: 2023-10-23 12:07:20 +10:30)\n"
     ]
    }
   ],
   "source": [
    "# Import Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Display elapsed time on Jupyter.\n",
    "%reload_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "\n",
    "def Adaboost_train(train_data, train_label, T):\n",
    "# train_data: N x d matrix\n",
    "# train_label: N x 1 vector\n",
    "# T: the number of weak classifiers in the ensemble\n",
    "    ensemble_models = []\n",
    "    for t in range(0,T):\n",
    "        model_param_t = weak_classifier_train(train_data, train_label) # model_param_t returns the model parameters of the learned weak classifier\n",
    "        # definition of model\n",
    "        ensemble_models.append(model_param_t)\n",
    "    return ensemble_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential Error\n",
    "\n",
    "1. No weight initialization: \n",
    "\n",
    "The AdaBoost algorithm first needs to initialize weights for each data point. These weights are used to train the weak classifier and are updated at each iteration. However, the code does not initialize or update these weights.\n",
    "\n",
    "2. No Weighted Error Calculation: \n",
    "\n",
    "After training a weak classifier, AdaBoost calculates the weighted error for that classifier. This is the sum of the weights of the misclassified examples. This step is missing from the code.\n",
    "\n",
    "3. No Alpha Calculation: \n",
    "\n",
    "AdaBoost calculates a weighted error alpha for each weak classifier, which is used to update the weights of the data points. alpha value also determines the weak classifier's contribution to the final set. This step is missing from the code.\n",
    "\n",
    "4. No weight update: \n",
    "\n",
    "After calculating the alpha value, AdaBoost updates the weights of the data points. The weights of the correctly categorized examples are decreased, while the weights of the incorrectly categorized examples are increased. This step is missing from the code.\n",
    "\n",
    "5. No final model output: AdaBoost merges weak classifiers into strong ones. The contribution of each weak classifier is determined by its alpha value. However, the code only returns the parameters of the weak classifiers and does not combine them into a final model.\n",
    "\n",
    "#### weak_classifier_train:\n",
    "\n",
    "- Input: \n",
    "\n",
    "The function should take in the training data, training labels, and the weights of the data points.\n",
    "\n",
    "- Output: \n",
    "\n",
    "It should return the model parameters of the learned weak classifier and possibly other relevant information. It should includes feature, threshold and polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try to correct Adaboost_train\n",
    "\n",
    "def Adaboost_train(train_data, train_label, T):\n",
    "    N, _ = train_data.shape\n",
    "    weights = np.ones(N) / N  # Initialize weights\n",
    "    ensemble_models = []\n",
    "    alphas = []\n",
    "\n",
    "    for t in range(T):\n",
    "        model_param_t = weak_classifier_train(train_data, train_label, weights)\n",
    "        predictions = weak_classifier_predict(train_data, model_param_t)\n",
    "        \n",
    "        # Calculate weighted error\n",
    "        error = np.sum(weights[predictions != train_label])\n",
    "        \n",
    "        # Calculate alpha\n",
    "        alpha = 0.5 * np.log((1 - error) / error)\n",
    "        alphas.append(alpha)\n",
    "        \n",
    "        # Update weights\n",
    "        weights[predictions == train_label] *= np.exp(-alpha)\n",
    "        weights[predictions != train_label] *= np.exp(alpha)\n",
    "        \n",
    "        # Normalize weights\n",
    "        weights /= np.sum(weights)\n",
    "        \n",
    "        ensemble_models.append(model_param_t)\n",
    "\n",
    "    return ensemble_models, alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.83 ms (started: 2023-10-23 12:07:26 +10:30)\n"
     ]
    }
   ],
   "source": [
    "## Implement Discrete AdaBoost\n",
    "import numpy as np\n",
    "\n",
    "def weak_classifier_train(X, y, weights):\n",
    "    \"\"\"Train a decision stump (weak classifier) using weighted data.\"\"\"\n",
    "    n, d = X.shape\n",
    "    best_error = float('inf')\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_polarity = 1\n",
    "\n",
    "    for feature in range(d):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            predictions = np.where(X[:, feature] < threshold, 1, -1)\n",
    "            error = np.sum(weights[predictions != y])\n",
    "\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "\n",
    "            # Check the other polarity\n",
    "            predictions = np.where(X[:, feature] > threshold, 1, -1)\n",
    "            error = np.sum(weights[predictions != y])\n",
    "\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                best_polarity = -1\n",
    "\n",
    "    return best_feature, best_threshold, best_polarity\n",
    "\n",
    "def weak_classifier_predict(X, feature, threshold, polarity):\n",
    "    \"\"\"Make predictions using a decision stump.\"\"\"\n",
    "    if polarity == 1:\n",
    "        return np.where(X[:, feature] < threshold, 1, -1)\n",
    "    else:\n",
    "        return np.where(X[:, feature] > threshold, 1, -1)\n",
    "\n",
    "def adaboost_train(X, y, T):\n",
    "    n, _ = X.shape\n",
    "    weights = np.full(n, 1/n)\n",
    "    alphas = []\n",
    "    classifiers = []\n",
    "\n",
    "    for t in range(T):\n",
    "        # Train a weak classifier\n",
    "        feature, threshold, polarity = weak_classifier_train(X, y, weights)\n",
    "        predictions = weak_classifier_predict(X, feature, threshold, polarity)\n",
    "\n",
    "        # Calculate the error of the weak classifier\n",
    "        error = np.sum(weights[predictions != y])\n",
    "\n",
    "        # Compute alpha\n",
    "        alpha = 0.5 * np.log((1 - error) / error)\n",
    "        alphas.append(alpha)\n",
    "        classifiers.append((feature, threshold, polarity))\n",
    "\n",
    "        # Update weights\n",
    "        weights[predictions == y] *= np.exp(-alpha)\n",
    "        weights[predictions != y] *= np.exp(alpha)\n",
    "\n",
    "        # Normalize weights\n",
    "        weights /= np.sum(weights)\n",
    "\n",
    "    return alphas, classifiers\n",
    "\n",
    "def adaboost_predict(X, alphas, classifiers):\n",
    "    \"\"\"Predict using the ensemble of weak classifiers.\"\"\"\n",
    "    n, _ = X.shape\n",
    "    predictions = np.zeros(n)\n",
    "\n",
    "    for alpha, (feature, threshold, polarity) in zip(alphas, classifiers):\n",
    "        predictions += alpha * weak_classifier_predict(X, feature, threshold, polarity)\n",
    "\n",
    "    return np.sign(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom AdaBoost Accuracy: 94.68%\n",
      "Sklearn AdaBoost Accuracy: 93.59%\n",
      "time: 27.4 s (started: 2023-10-23 13:37:41 +10:30)\n"
     ]
    }
   ],
   "source": [
    "## Verify the adaboost\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Spambase dataset with parser set to 'auto'\n",
    "data = fetch_openml('spambase', version=1, parser='auto')\n",
    "X, y = data['data'], data['target']\n",
    "y = np.where(y == '1', 1, -1)  # Convert labels to 1 and -1\n",
    "\n",
    "# If the data is in pandas DataFrame format, convert it to numpy arrays\n",
    "if hasattr(X, 'values'):\n",
    "    X = X.values\n",
    "if hasattr(y, 'values'):\n",
    "    y = y.values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the custom AdaBoost implementation\n",
    "alphas, classifiers = adaboost_train(X_train, y_train, T=50)\n",
    "custom_predictions = adaboost_predict(X_test, alphas, classifiers)\n",
    "\n",
    "# Train the sklearn AdaBoost implementation\n",
    "clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "sklearn_predictions = clf.predict(X_test)\n",
    "\n",
    "# Compare the accuracy of both implementations\n",
    "custom_accuracy = accuracy_score(y_test, custom_predictions)\n",
    "sklearn_accuracy = accuracy_score(y_test, sklearn_predictions)\n",
    "\n",
    "print(f\"Custom AdaBoost Accuracy: {custom_accuracy * 100:.2f}%\")\n",
    "print(f\"Sklearn AdaBoost Accuracy: {sklearn_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
